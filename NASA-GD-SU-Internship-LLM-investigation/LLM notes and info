MPT(MosaicML Pretrained Transformer):
  trained in 9.5 days with no human intervention
  goal is to provide a commercially-usable, open-source model 
  can train, fintunre and delpoy private MPT models 
  decoder-style transformer; modified transofrmer architecture optimized for efficient training and interference
  eliminates context length limits via positional embeddings 
  Liscensed for commercial use unlike LLaMa-7B
  trained on 1T of tokens
  can handle long inputs
  fast training capabilities
  matches the quality of LLaMA-7B
  Models Finetuned off MPT-7B:
    MPT-7BStoryWriter-65k+: designed to read and write fictional stories
    MPT-7B-Instruct: short-form instruction following
    MPT-7B-Chat: chatbot model for dialgoue generation
  Build and train: https://www.mosaicml.com/get-started?utm_source=mosaicml-blog&utm_medium=mosaicml.com&utm_campaign=mpt-7b

Open Assistant:
  goal: LLM tha can run on a single GPU; make third party integration easy; 
  nFastAPI is used by web front end and the Discord bots to store conversation trees and the netadate
  users host the model themselves for interfence on their own hardware 
  tried using their chatbot and it was not loading the answers to my question and half way through the waiting process it logged itself out
  Uses Reinforcement Learning with human Feedback (RLHF)
  use pre-defined converstaions between assitant and human to train an existing LLM [Supervised Fine-Tuning Model (SFT)] 
    convos are generated by the human acting as the prompterand as the assistant
  a conversation tree is generated
  human ranks the answers from the assistant [Reward Model(RM)]
  optimize a policy against the reward model using Reinforcement Learning (RL)
  all information gathered from this site: https://projects.laion.ai/Open-Assistant/docs/guides/developers

LLaMA (Large Language Model Meta AI):
  released by Meta
  avaiable in several sizes (7B, 13B, 33B and 65B parameters)
  access to LLaMA must be applied for 
  



  

